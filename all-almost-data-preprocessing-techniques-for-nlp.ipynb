{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84660ad2",
   "metadata": {
    "papermill": {
     "duration": 0.018025,
     "end_time": "2023-02-16T05:28:28.062581",
     "exception": false,
     "start_time": "2023-02-16T05:28:28.044556",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa57f24",
   "metadata": {
    "papermill": {
     "duration": 0.016729,
     "end_time": "2023-02-16T05:28:28.096143",
     "exception": false,
     "start_time": "2023-02-16T05:28:28.079414",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "This notebook is an attempt to present different text preprocessing techniques used in natural language processing (NLP) at one place for better understanding and as a single point of reference. I have tried to make it as comprehensive as possible and this notebook will be updated time to time to include additional methods or update existing methods if any other more effecient approaches are availble to achieve the same task. Few of the methods included here are specific to the tweets dataset such as tweeter mentions while others can be used for any NLP dataset. \n",
    "\n",
    "### Why to do text preprocessing\n",
    "All the machine learning models available today only work on numerical values and do not understand the text as is. So we have to represent the textual information using numerical values. The textual data generated from natural languaes is inherently unstructured and noisy. To represent the text data, we have to clean it to remove parts of the text which do not add any information, such as punctuations and stopwords, to make the data more consistent. Such consistent data can be represented efficiently for the use in machine learning models.   \n",
    "\n",
    "### Methods used for text preprocessing\n",
    "There are number of methods used for preprocessing the textual data and their use depends upon the data itself as well as the application. For example, the data with human communication such as emails or tweets will have many contractions of words like can't for can not or they've for they have. On the other hand, such contractions might not be present in a formal document such as a scientific journal article and their expansion will not be a necessary step in text preprocessing. \n",
    "Here are different methods included in this notebook.\n",
    "1. Lower Casing\n",
    "2. Removing HTML\n",
    "3. Expand Contractions\n",
    "4. Removing URLs\n",
    "5. Removing Email IDs\n",
    "6. Removing Tweeter Mentions\n",
    "7. Handling Emojis\n",
    "8. Handling Accented Words\n",
    "8. Removing Unicode Characters\n",
    "9. Abbreviation/Acronym Disambiguation\n",
    "10. Removing Digits or Words with Digits\n",
    "11. Removing Stopwords\n",
    "12. Removing Extra Spaces\n",
    "13. Stemming or Lemmatization\n",
    "14. Spelling Correction\n",
    "15. Correcting Compound Words\n",
    "\n",
    "A breif explanation about each method is included in the subsections and the sequence of these preprocessing steps is discussed at the end.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841d6504",
   "metadata": {
    "papermill": {
     "duration": 0.016676,
     "end_time": "2023-02-16T05:28:28.129530",
     "exception": false,
     "start_time": "2023-02-16T05:28:28.112854",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Why to keep hashtags in Tweets data\n",
    "The hashtags in tweets is a way to include keywords or key phrases for better discoverability on Tweeter. Moreover, they also provide a general context of the contents of the tweet and the topic it is related to. For example, there are hashtags like #wildfire, #flooding and #caraccident in the disaster tweets dataset, which mention a disaster, which is exactly what we are trying to classify. So if the tweet contains such hashtag, it is more likely to be related to the particular disaster witnessed by the person tweeting it. Hence keeping the hashtags will include more information (sometimes more than the tweet itself).    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f09cd8e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-16T05:28:28.164931Z",
     "iopub.status.busy": "2023-02-16T05:28:28.164396Z",
     "iopub.status.idle": "2023-02-16T05:28:29.823740Z",
     "shell.execute_reply": "2023-02-16T05:28:29.822117Z"
    },
    "papermill": {
     "duration": 1.680792,
     "end_time": "2023-02-16T05:28:29.826919",
     "exception": false,
     "start_time": "2023-02-16T05:28:28.146127",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='bs4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d0555b",
   "metadata": {
    "papermill": {
     "duration": 0.02212,
     "end_time": "2023-02-16T05:28:29.870273",
     "exception": false,
     "start_time": "2023-02-16T05:28:29.848153",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ade80f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-16T05:28:29.907713Z",
     "iopub.status.busy": "2023-02-16T05:28:29.906910Z",
     "iopub.status.idle": "2023-02-16T05:28:29.979267Z",
     "shell.execute_reply": "2023-02-16T05:28:29.978496Z"
    },
    "papermill": {
     "duration": 0.09439,
     "end_time": "2023-02-16T05:28:29.982311",
     "exception": false,
     "start_time": "2023-02-16T05:28:29.887921",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\n",
    "tweets_df.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660cfd24",
   "metadata": {
    "papermill": {
     "duration": 0.018416,
     "end_time": "2023-02-16T05:28:30.021460",
     "exception": false,
     "start_time": "2023-02-16T05:28:30.003044",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "If we check the description of the competition, we can observe that the keywords are important for the classification of distaster tweet and hence a combined tweet column is created by joining keyword and text. First the empty keywords are replaced by \"\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c1aaa01",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-16T05:28:30.057925Z",
     "iopub.status.busy": "2023-02-16T05:28:30.057361Z",
     "iopub.status.idle": "2023-02-16T05:28:30.079432Z",
     "shell.execute_reply": "2023-02-16T05:28:30.078515Z"
    },
    "papermill": {
     "duration": 0.042514,
     "end_time": "2023-02-16T05:28:30.081210",
     "exception": false,
     "start_time": "2023-02-16T05:28:30.038696",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2644</th>\n",
       "      <td>3796</td>\n",
       "      <td>destruction</td>\n",
       "      <td>NaN</td>\n",
       "      <td>So you have a new weapon that can cause un-ima...</td>\n",
       "      <td>1</td>\n",
       "      <td>destruction So you have a new weapon that can ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2227</th>\n",
       "      <td>3185</td>\n",
       "      <td>deluge</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The f$&amp;amp;@ing things I do for #GISHWHES Just...</td>\n",
       "      <td>0</td>\n",
       "      <td>deluge The f$&amp;amp;@ing things I do for #GISHWH...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5448</th>\n",
       "      <td>7769</td>\n",
       "      <td>police</td>\n",
       "      <td>UK</td>\n",
       "      <td>DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe...</td>\n",
       "      <td>1</td>\n",
       "      <td>police DT @georgegalloway: RT @Galloway4Mayor:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>191</td>\n",
       "      <td>aftershock</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Aftershock back to school kick off was great. ...</td>\n",
       "      <td>0</td>\n",
       "      <td>aftershock Aftershock back to school kick off ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6845</th>\n",
       "      <td>9810</td>\n",
       "      <td>trauma</td>\n",
       "      <td>Montgomery County, MD</td>\n",
       "      <td>in response to trauma Children of Addicts deve...</td>\n",
       "      <td>0</td>\n",
       "      <td>trauma in response to trauma Children of Addic...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id      keyword               location  \\\n",
       "2644  3796  destruction                    NaN   \n",
       "2227  3185       deluge                    NaN   \n",
       "5448  7769       police                     UK   \n",
       "132    191   aftershock                    NaN   \n",
       "6845  9810       trauma  Montgomery County, MD   \n",
       "\n",
       "                                                   text  target  \\\n",
       "2644  So you have a new weapon that can cause un-ima...       1   \n",
       "2227  The f$&amp;@ing things I do for #GISHWHES Just...       0   \n",
       "5448  DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe...       1   \n",
       "132   Aftershock back to school kick off was great. ...       0   \n",
       "6845  in response to trauma Children of Addicts deve...       0   \n",
       "\n",
       "                                                  tweet  \n",
       "2644  destruction So you have a new weapon that can ...  \n",
       "2227  deluge The f$&amp;@ing things I do for #GISHWH...  \n",
       "5448  police DT @georgegalloway: RT @Galloway4Mayor:...  \n",
       "132   aftershock Aftershock back to school kick off ...  \n",
       "6845  trauma in response to trauma Children of Addic...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df[\"keyword\"] = tweets_df[\"keyword\"].fillna(\"\")\n",
    "tweets_df[\"tweet\"] = tweets_df[\"keyword\"] + \" \" + tweets_df[\"text\"]\n",
    "tweets_df.sample(5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe07cc1",
   "metadata": {
    "papermill": {
     "duration": 0.016737,
     "end_time": "2023-02-16T05:28:30.114851",
     "exception": false,
     "start_time": "2023-02-16T05:28:30.098114",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Lower Case\n",
    "Lower casing is generally the first preprocessing step performed. As each word is considered as a token, if we consider all the possible combinations of the casings of the word, it will be $2^{(len(word))}$ possible different tokens for each word, like dataset, Dataset, DaTaSeT and so on. We want to represent the text as efficiently as possible and hence want to make the representation case insentitve, as generally the casing does not provide any addditional information. Exception to this are the abbreviations, which are handled separately.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "231a32be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-16T05:28:30.151003Z",
     "iopub.status.busy": "2023-02-16T05:28:30.150442Z",
     "iopub.status.idle": "2023-02-16T05:28:30.162467Z",
     "shell.execute_reply": "2023-02-16T05:28:30.161572Z"
    },
    "papermill": {
     "duration": 0.032244,
     "end_time": "2023-02-16T05:28:30.164352",
     "exception": false,
     "start_time": "2023-02-16T05:28:30.132108",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2644    destruction so you have a new weapon that can ...\n",
       "2227    deluge the f$&amp;@ing things i do for #gishwh...\n",
       "5448    police dt @georgegalloway: rt @galloway4mayor:...\n",
       "132     aftershock aftershock back to school kick off ...\n",
       "6845    trauma in response to trauma children of addic...\n",
       "Name: tweet_lower, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df[\"tweet_lower\"] = tweets_df[\"tweet\"].str.lower()\n",
    "tweets_df[\"tweet_lower\"].sample(5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe57e825",
   "metadata": {
    "papermill": {
     "duration": 0.016654,
     "end_time": "2023-02-16T05:28:30.198009",
     "exception": false,
     "start_time": "2023-02-16T05:28:30.181355",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Remove HTML\n",
    "HTML stands for HyperText Markup Language, which is used for formatting the flow of the webpages. There are many html entities, which creep into the textual data such as \"& gt;\" and \"& lt;\". Also text might contain html tags such as < p >, < a > or < div >. It is important to remove these entities as they are nothing but noise and can negatively affect the performance of the model if not removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a7f2a64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-16T05:28:30.233848Z",
     "iopub.status.busy": "2023-02-16T05:28:30.233275Z",
     "iopub.status.idle": "2023-02-16T05:28:30.423866Z",
     "shell.execute_reply": "2023-02-16T05:28:30.422735Z"
    },
    "papermill": {
     "duration": 0.211493,
     "end_time": "2023-02-16T05:28:30.426457",
     "exception": false,
     "start_time": "2023-02-16T05:28:30.214964",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'>> $15 Aftershock : Protect Yourself and Profit in the Next Global Financial... ##book http://t.co/f6ntUc734Z esquireattire'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "text = r\"&gt;&gt; $15 Aftershock : Protect Yourself and Profit in the Next Global Financial... ##book http://t.co/f6ntUc734Z esquireattire\"\n",
    "soup = BeautifulSoup(text)\n",
    "soup.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c81f20d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-16T05:28:30.463789Z",
     "iopub.status.busy": "2023-02-16T05:28:30.461642Z",
     "iopub.status.idle": "2023-02-16T05:28:30.467996Z",
     "shell.execute_reply": "2023-02-16T05:28:30.467134Z"
    },
    "papermill": {
     "duration": 0.026545,
     "end_time": "2023-02-16T05:28:30.469991",
     "exception": false,
     "start_time": "2023-02-16T05:28:30.443446",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_html(text):\n",
    "    soup = BeautifulSoup(text)\n",
    "    text = soup.get_text()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c6381a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-16T05:28:30.505972Z",
     "iopub.status.busy": "2023-02-16T05:28:30.505622Z",
     "iopub.status.idle": "2023-02-16T05:28:31.975457Z",
     "shell.execute_reply": "2023-02-16T05:28:31.973872Z"
    },
    "papermill": {
     "duration": 1.490109,
     "end_time": "2023-02-16T05:28:31.977379",
     "exception": false,
     "start_time": "2023-02-16T05:28:30.487270",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2644    destruction so you have a new weapon that can ...\n",
       "2227    deluge the f$&@ing things i do for #gishwhes j...\n",
       "5448    police dt @georgegalloway: rt @galloway4mayor:...\n",
       "132     aftershock aftershock back to school kick off ...\n",
       "6845    trauma in response to trauma children of addic...\n",
       "Name: tweet_noHTML, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df[\"tweet_noHTML\"] = tweets_df[\"tweet_lower\"].apply(remove_html)\n",
    "tweets_df[\"tweet_noHTML\"].sample(5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24517afb",
   "metadata": {
    "papermill": {
     "duration": 0.016999,
     "end_time": "2023-02-16T05:28:32.011610",
     "exception": false,
     "start_time": "2023-02-16T05:28:31.994611",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Expand Contractions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659374f5",
   "metadata": {
    "papermill": {
     "duration": 0.017048,
     "end_time": "2023-02-16T05:28:32.045901",
     "exception": false,
     "start_time": "2023-02-16T05:28:32.028853",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "There are many contractions of words used in informal communication such as can't: can not, they've: they have or even modern contractions such as sux: sucks. In many cases, these contractions are considered as stopwords and are removed. There is a python package to expand such contractions conveniently named as `contractions`, which has collection of most of such contractions and can be used for expanding them as a preprocessing step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "526e8a9b",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2023-02-16T05:28:32.082984Z",
     "iopub.status.busy": "2023-02-16T05:28:32.081901Z",
     "iopub.status.idle": "2023-02-16T05:28:44.826696Z",
     "shell.execute_reply": "2023-02-16T05:28:44.825837Z"
    },
    "papermill": {
     "duration": 12.766072,
     "end_time": "2023-02-16T05:28:44.829304",
     "exception": false,
     "start_time": "2023-02-16T05:28:32.063232",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting contractions\r\n",
      "  Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\r\n",
      "Collecting textsearch>=0.0.21\r\n",
      "  Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\r\n",
      "Collecting anyascii\r\n",
      "  Downloading anyascii-0.3.1-py3-none-any.whl (287 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.5/287.5 kB\u001b[0m \u001b[31m888.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting pyahocorasick\r\n",
      "  Downloading pyahocorasick-2.0.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (101 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.8/101.8 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\r\n",
      "Successfully installed anyascii-0.3.1 contractions-0.1.73 pyahocorasick-2.0.0 textsearch-0.0.24\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c6d0ca2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-16T05:28:44.868999Z",
     "iopub.status.busy": "2023-02-16T05:28:44.868538Z",
     "iopub.status.idle": "2023-02-16T05:28:44.962678Z",
     "shell.execute_reply": "2023-02-16T05:28:44.961599Z"
    },
    "papermill": {
     "duration": 0.117819,
     "end_time": "2023-02-16T05:28:44.965684",
     "exception": false,
     "start_time": "2023-02-16T05:28:44.847865",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2644    destruction so you have a new weapon that can ...\n",
       "2227    deluge the f$&@ing things i do for #gishwhes j...\n",
       "5448    police dt @georgegalloway: rt @galloway4mayor:...\n",
       "132     aftershock aftershock back to school kick off ...\n",
       "6845    trauma in response to trauma children of addic...\n",
       "Name: tweet_noContractions, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import contractions\n",
    "\n",
    "tweets_df[\"tweet_noContractions\"] = tweets_df[\"tweet_noHTML\"].apply(contractions.fix)\n",
    "tweets_df[\"tweet_noContractions\"].sample(5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43240ed7",
   "metadata": {
    "papermill": {
     "duration": 0.017675,
     "end_time": "2023-02-16T05:28:45.001672",
     "exception": false,
     "start_time": "2023-02-16T05:28:44.983997",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Remove URLs\n",
    "URL stands for Uniform Resource Locator, which is used to locate resources on the web. However, they generally do not provide any additional information in the NLP task and are hard to handle otherwise. Hence they need to be removed. All the URLs can be completely removed or can be replaced by some common word such as 'website' or 'url' to keep the information about the presense of URL in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b635499e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-16T05:28:45.039931Z",
     "iopub.status.busy": "2023-02-16T05:28:45.039489Z",
     "iopub.status.idle": "2023-02-16T05:28:45.044289Z",
     "shell.execute_reply": "2023-02-16T05:28:45.043393Z"
    },
    "papermill": {
     "duration": 0.027366,
     "end_time": "2023-02-16T05:28:45.047096",
     "exception": false,
     "start_time": "2023-02-16T05:28:45.019730",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_urls(text):\n",
    "    pattern = re.compile(r'https?://(www\\.)?(\\w+)(\\.\\w+)(/\\w*)?')\n",
    "    text = re.sub(pattern, \"\", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13080771",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-16T05:28:45.086115Z",
     "iopub.status.busy": "2023-02-16T05:28:45.085726Z",
     "iopub.status.idle": "2023-02-16T05:28:45.091989Z",
     "shell.execute_reply": "2023-02-16T05:28:45.091046Z"
    },
    "papermill": {
     "duration": 0.028108,
     "end_time": "2023-02-16T05:28:45.093903",
     "exception": false,
     "start_time": "2023-02-16T05:28:45.065795",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#stlouis #caraccidentlawyer Speeding Among Top Causes of Teen Accidents   Car Accident'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"#stlouis #caraccidentlawyer Speeding Among Top Causes of Teen Accidents https://t.co/k4zoMOF319 https://t.co/S2kXVM0cBA Car Accident\"\n",
    "remove_urls(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d2dfdf8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-16T05:28:45.132869Z",
     "iopub.status.busy": "2023-02-16T05:28:45.132478Z",
     "iopub.status.idle": "2023-02-16T05:28:45.166264Z",
     "shell.execute_reply": "2023-02-16T05:28:45.164700Z"
    },
    "papermill": {
     "duration": 0.05574,
     "end_time": "2023-02-16T05:28:45.168223",
     "exception": false,
     "start_time": "2023-02-16T05:28:45.112483",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2644    destruction so you have a new weapon that can ...\n",
       "2227    deluge the f$&@ing things i do for #gishwhes j...\n",
       "5448    police dt @georgegalloway: rt @galloway4mayor:...\n",
       "132     aftershock aftershock back to school kick off ...\n",
       "6845    trauma in response to trauma children of addic...\n",
       "Name: tweet_noURLs, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df[\"tweet_noURLs\"] = tweets_df[\"tweet_noContractions\"].apply(remove_urls)\n",
    "tweets_df[\"tweet_noURLs\"].sample(5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2631d7",
   "metadata": {
    "papermill": {
     "duration": 0.018207,
     "end_time": "2023-02-16T05:28:45.204920",
     "exception": false,
     "start_time": "2023-02-16T05:28:45.186713",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Remove Email IDs\n",
    "Email ids have become ubiquitous over the years and appear everywhere. As they do not provide any additional information (unless you are specifically extracting the emails from the text for specific usecase) we need to remove them. Similar to the previous case, email id can be completely removed or replaced with a common word such as \"email\".  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9625fd3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-16T05:28:45.245857Z",
     "iopub.status.busy": "2023-02-16T05:28:45.244768Z",
     "iopub.status.idle": "2023-02-16T05:28:45.251363Z",
     "shell.execute_reply": "2023-02-16T05:28:45.249868Z"
    },
    "papermill": {
     "duration": 0.030157,
     "end_time": "2023-02-16T05:28:45.253982",
     "exception": false,
     "start_time": "2023-02-16T05:28:45.223825",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_emails(text):\n",
    "    pattern = re.compile(r\"[\\w\\.-]+@[\\w\\.-]+\\.\\w+\")\n",
    "    text = re.sub(pattern, \"\", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1523bcda",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-16T05:28:45.294823Z",
     "iopub.status.busy": "2023-02-16T05:28:45.294120Z",
     "iopub.status.idle": "2023-02-16T05:28:45.300437Z",
     "shell.execute_reply": "2023-02-16T05:28:45.299173Z"
    },
    "papermill": {
     "duration": 0.029618,
     "end_time": "2023-02-16T05:28:45.302799",
     "exception": false,
     "start_time": "2023-02-16T05:28:45.273181",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'please send your feedback to  '"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"please send your feedback to myemail@gmail.com \"\n",
    "remove_emails(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "13bb5546",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-16T05:28:45.343534Z",
     "iopub.status.busy": "2023-02-16T05:28:45.342812Z",
     "iopub.status.idle": "2023-02-16T05:28:45.400332Z",
     "shell.execute_reply": "2023-02-16T05:28:45.399442Z"
    },
    "papermill": {
     "duration": 0.079714,
     "end_time": "2023-02-16T05:28:45.402297",
     "exception": false,
     "start_time": "2023-02-16T05:28:45.322583",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2644    destruction so you have a new weapon that can ...\n",
       "2227    deluge the f$&@ing things i do for #gishwhes j...\n",
       "5448    police dt @georgegalloway: rt @galloway4mayor:...\n",
       "132     aftershock aftershock back to school kick off ...\n",
       "6845    trauma in response to trauma children of addic...\n",
       "Name: tweet_noEmail, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df[\"tweet_noEmail\"] = tweets_df[\"tweet_noURLs\"].apply(remove_emails)\n",
    "tweets_df[\"tweet_noEmail\"].sample(5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2dc250",
   "metadata": {
    "papermill": {
     "duration": 0.018718,
     "end_time": "2023-02-16T05:28:45.440298",
     "exception": false,
     "start_time": "2023-02-16T05:28:45.421580",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Remove Tweeter Mentions\n",
    "The text contains maintions using @. This generally appears in Tweeter and online forums. We need to remove these mentions before removing the punctutions otherwise they will be hard to find without the @ attached to it. They are generally names of people and don't provide any additional information helpful for the NLP task. (Exception to this can be the News channel handles in Disaster Tweets dataset, which can be handled separately if required.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d2f0a449",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-16T05:28:45.480428Z",
     "iopub.status.busy": "2023-02-16T05:28:45.479383Z",
     "iopub.status.idle": "2023-02-16T05:28:45.484488Z",
     "shell.execute_reply": "2023-02-16T05:28:45.483518Z"
    },
    "papermill": {
     "duration": 0.028472,
     "end_time": "2023-02-16T05:28:45.487726",
     "exception": false,
     "start_time": "2023-02-16T05:28:45.459254",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_mentions(text):\n",
    "    pattern = re.compile(r\"@\\w+\")\n",
    "    text = re.sub(pattern, \"\", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f9e3180",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-16T05:28:45.528837Z",
     "iopub.status.busy": "2023-02-16T05:28:45.527494Z",
     "iopub.status.idle": "2023-02-16T05:28:45.555952Z",
     "shell.execute_reply": "2023-02-16T05:28:45.555053Z"
    },
    "papermill": {
     "duration": 0.050994,
     "end_time": "2023-02-16T05:28:45.558000",
     "exception": false,
     "start_time": "2023-02-16T05:28:45.507006",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2644    destruction so you have a new weapon that can ...\n",
       "2227    deluge the f$& things i do for #gishwhes just ...\n",
       "5448    police dt : rt : ûïthe col police can catch a...\n",
       "132     aftershock aftershock back to school kick off ...\n",
       "6845    trauma in response to trauma children of addic...\n",
       "Name: tweet_noMention, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df[\"tweet_noMention\"] = tweets_df[\"tweet_noEmail\"].apply(remove_mentions)\n",
    "tweets_df[\"tweet_noMention\"].sample(5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e26b765",
   "metadata": {
    "papermill": {
     "duration": 0.018783,
     "end_time": "2023-02-16T05:28:45.595518",
     "exception": false,
     "start_time": "2023-02-16T05:28:45.576735",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Hashtags can also be removed in similar way but in this competition the hashtags are important as they include key information hence are not removed as mentioned before."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac8d586",
   "metadata": {
    "papermill": {
     "duration": 0.019408,
     "end_time": "2023-02-16T05:28:45.633968",
     "exception": false,
     "start_time": "2023-02-16T05:28:45.614560",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Handling Emojis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76af2a9b",
   "metadata": {
    "papermill": {
     "duration": 0.01966,
     "end_time": "2023-02-16T05:28:45.673815",
     "exception": false,
     "start_time": "2023-02-16T05:28:45.654155",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Generally emojis are removed, but in the case of distaster tweets, the emojis can contain some information and hence need to be handled properly.\n",
    "\n",
    "I propose to convert the emojis to six basic emotions such as happiness, sadness, anger, disgust, fear, surprise and the neutral state. Each emotion class can contain multiple emojis such as happiness can contain 😀 😃 😄 😁 😆 😅 😂 🤣. Other possible approach might be to simply replace replace the emoji by the name of emoji such as replace 😀 by \"smily face\". \n",
    "\n",
    "This step needs to be done before removing Unicode characters in the next step because emojis are represented in unicode. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9c34a1",
   "metadata": {
    "papermill": {
     "duration": 0.01895,
     "end_time": "2023-02-16T05:28:45.712322",
     "exception": false,
     "start_time": "2023-02-16T05:28:45.693372",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**I think I will wait for some discussion in the comments regarding this before implementing any approach for this, because there can be ambiguity in the use of emojis as well**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0976bf5c",
   "metadata": {
    "papermill": {
     "duration": 0.018956,
     "end_time": "2023-02-16T05:28:45.751008",
     "exception": false,
     "start_time": "2023-02-16T05:28:45.732052",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Handling Accented Words\n",
    "Accent marks (accents) in English is largely confined to proper names or “borrowed” words of foreign origin, such as résumé and tête-à-tête, they occur frequently in several other European languages, including Spanish, French, Italian, German and Portuguese. We have to handle these accented characters before removing unicode characters, otherwise they will get removed. This may not may not be helpful in Distaster tweets classification task as text data is too noisy and contain many unicode characters which can get converted into ASCII characters creating giberish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8ebace0b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-16T05:28:45.791892Z",
     "iopub.status.busy": "2023-02-16T05:28:45.790505Z",
     "iopub.status.idle": "2023-02-16T05:28:45.802944Z",
     "shell.execute_reply": "2023-02-16T05:28:45.802018Z"
    },
    "papermill": {
     "duration": 0.034906,
     "end_time": "2023-02-16T05:28:45.805103",
     "exception": false,
     "start_time": "2023-02-16T05:28:45.770197",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'words of foreign origin, such as resume and tete-a-tete'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from unidecode import unidecode\n",
    "text = \"words of foreign origin, such as résumé and tête-à-tête\"\n",
    "unidecode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "25b48f56",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-16T05:28:45.845576Z",
     "iopub.status.busy": "2023-02-16T05:28:45.845189Z",
     "iopub.status.idle": "2023-02-16T05:28:45.850640Z",
     "shell.execute_reply": "2023-02-16T05:28:45.849032Z"
    },
    "papermill": {
     "duration": 0.02854,
     "end_time": "2023-02-16T05:28:45.853048",
     "exception": false,
     "start_time": "2023-02-16T05:28:45.824508",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def handle_accents(text):\n",
    "    text = unidecode(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e568e300",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-16T05:28:45.895286Z",
     "iopub.status.busy": "2023-02-16T05:28:45.894677Z",
     "iopub.status.idle": "2023-02-16T05:28:45.942034Z",
     "shell.execute_reply": "2023-02-16T05:28:45.941297Z"
    },
    "papermill": {
     "duration": 0.071325,
     "end_time": "2023-02-16T05:28:45.944232",
     "exception": false,
     "start_time": "2023-02-16T05:28:45.872907",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2644    destruction so you have a new weapon that can ...\n",
       "2227    deluge the f$& things i do for #gishwhes just ...\n",
       "5448    police dt : rt : uithe col police can catch a ...\n",
       "132     aftershock aftershock back to school kick off ...\n",
       "6845    trauma in response to trauma children of addic...\n",
       "Name: tweet_handleAccents, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df[\"tweet_handleAccents\"] = tweets_df[\"tweet_noMention\"].apply(handle_accents)\n",
    "tweets_df[\"tweet_handleAccents\"].sample(5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee09d7b",
   "metadata": {
    "papermill": {
     "duration": 0.019321,
     "end_time": "2023-02-16T05:28:45.982902",
     "exception": false,
     "start_time": "2023-02-16T05:28:45.963581",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Remove Unicode Charachers\n",
    "> Unicode, formally The Unicode Standard, is an information technology standard for the consistent encoding, representation, and handling of text expressed in most of the world's writing systems. The standard, which is maintained by the Unicode Consortium, defines as of the current version (15.0) 149,186 characters covering 161 modern and historic scripts, as well as symbols, 3664 emoji (including in colors), and non-visual control and formatting codes. - [Wikipedia](https://en.wikipedia.org/wiki/Unicode)\n",
    "\n",
    "On the other hand, the ASCII character set has only 128 characters which include upper and lower case english letters, numbers, few punctuations and control characters. In many cases we need to represent the text in only in alphanumeric characters and hence need to remove the other unicode characters. (This might not be true while working with languages other than English)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8df3a7db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-16T05:28:46.024200Z",
     "iopub.status.busy": "2023-02-16T05:28:46.023759Z",
     "iopub.status.idle": "2023-02-16T05:28:46.029777Z",
     "shell.execute_reply": "2023-02-16T05:28:46.028271Z"
    },
    "papermill": {
     "duration": 0.029582,
     "end_time": "2023-02-16T05:28:46.032042",
     "exception": false,
     "start_time": "2023-02-16T05:28:46.002460",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_unicode_chars(text):\n",
    "    text = text.encode(\"ascii\", \"ignore\").decode()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef7bc2f",
   "metadata": {
    "papermill": {
     "duration": 0.018885,
     "end_time": "2023-02-16T05:28:46.071150",
     "exception": false,
     "start_time": "2023-02-16T05:28:46.052265",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "As mentioned bbefore, the accented characters are removed by this step. For example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "480b81a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-16T05:28:46.111809Z",
     "iopub.status.busy": "2023-02-16T05:28:46.110867Z",
     "iopub.status.idle": "2023-02-16T05:28:46.117487Z",
     "shell.execute_reply": "2023-02-16T05:28:46.116529Z"
    },
    "papermill": {
     "duration": 0.029234,
     "end_time": "2023-02-16T05:28:46.119452",
     "exception": false,
     "start_time": "2023-02-16T05:28:46.090218",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'words of foreign origin, such as rsum and tte--tte'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"words of foreign origin, such as résumé and tête-à-tête\"\n",
    "remove_unicode_chars(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3048d97e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-16T05:28:46.161941Z",
     "iopub.status.busy": "2023-02-16T05:28:46.161269Z",
     "iopub.status.idle": "2023-02-16T05:28:46.175325Z",
     "shell.execute_reply": "2023-02-16T05:28:46.174620Z"
    },
    "papermill": {
     "duration": 0.038155,
     "end_time": "2023-02-16T05:28:46.177140",
     "exception": false,
     "start_time": "2023-02-16T05:28:46.138985",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2644    destruction so you have a new weapon that can ...\n",
       "2227    deluge the f$& things i do for #gishwhes just ...\n",
       "5448    police dt : rt : the col police can catch a pi...\n",
       "132     aftershock aftershock back to school kick off ...\n",
       "6845    trauma in response to trauma children of addic...\n",
       "Name: tweet_noUnicode, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df[\"tweet_noUnicode\"] = tweets_df[\"tweet_noMention\"].apply(remove_unicode_chars)\n",
    "tweets_df[\"tweet_noUnicode\"].sample(5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ada5cc",
   "metadata": {
    "papermill": {
     "duration": 0.019073,
     "end_time": "2023-02-16T05:28:46.215678",
     "exception": false,
     "start_time": "2023-02-16T05:28:46.196605",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Abbreviation/Acronym Disambiguation\n",
    "There are large number of abbreviations and acronyms used in the text. These abbreviations can contain meaningful information for the classification task and might get removed or destorted during other preprocessing steps and hence they need to be expanded earlier in the preprocessing. @gunesevitan has given many of these abbreviations in his [notebook](https://www.kaggle.com/code/gunesevitan/nlp-with-disaster-tweets-eda-cleaning-and-bert). I am also trying to come up with some approach to find all such abbreviations in the Disaster Tweets dataset in notebook about [Abbreviation Disambiguation](https://www.kaggle.com/code/rohitgarud/abbreviation-disambiguation-for-disaster-tweets?rvi=1). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a97b700c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-16T05:28:46.256351Z",
     "iopub.status.busy": "2023-02-16T05:28:46.255672Z",
     "iopub.status.idle": "2023-02-16T05:28:46.262673Z",
     "shell.execute_reply": "2023-02-16T05:28:46.261696Z"
    },
    "papermill": {
     "duration": 0.030316,
     "end_time": "2023-02-16T05:28:46.265245",
     "exception": false,
     "start_time": "2023-02-16T05:28:46.234929",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Acronyms\n",
    "def remove_abbreviations(text):\n",
    "    text = re.sub(r\"mh370\", \"missing malaysia airlines flight\", text)\n",
    "    text = re.sub(r\"okwx\", \"oklahoma city weather\", text)\n",
    "    text = re.sub(r\"arwx\", \"arkansas weather\", text)    \n",
    "    text = re.sub(r\"gawx\", \"georgia weather\", text)  \n",
    "    text = re.sub(r\"scwx\", \"south carolina weather\", text)  \n",
    "    text = re.sub(r\"cawx\", \"california weather\", text)\n",
    "    text = re.sub(r\"tnwx\", \"tennessee weather\", text)\n",
    "    text = re.sub(r\"azwx\", \"arizona weather\", text)  \n",
    "    text = re.sub(r\"alwx\", \"alabama Weather\", text)\n",
    "    text = re.sub(r\"wordpressdotcom\", \"wordpress\", text)    \n",
    "    text = re.sub(r\"usnwsgov\", \"united states national weather service\", text)\n",
    "    text = re.sub(r\"suruc\", \"sanliurfa\", tweet)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d9fe14",
   "metadata": {
    "papermill": {
     "duration": 0.018742,
     "end_time": "2023-02-16T05:28:46.303224",
     "exception": false,
     "start_time": "2023-02-16T05:28:46.284482",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "There are many more abbreviations in the dataset and a more thorough checking is required to find all the abbreviations/acronyms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e80b65",
   "metadata": {
    "papermill": {
     "duration": 0.018402,
     "end_time": "2023-02-16T05:28:46.340662",
     "exception": false,
     "start_time": "2023-02-16T05:28:46.322260",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Remove Punctuations\n",
    "Punctuations are used for defining the structure of the text such as full stops for terminating the sentences. They can be used for sentense tokenization. However, in some NLP tasks, punctuations do not provide any relevant information and need to be removed. There are a number of ways of removing punctuations. The built-in regular expression library from Python is used for removing punctuations.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "94395516",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-16T05:28:46.382393Z",
     "iopub.status.busy": "2023-02-16T05:28:46.381788Z",
     "iopub.status.idle": "2023-02-16T05:28:46.387288Z",
     "shell.execute_reply": "2023-02-16T05:28:46.386595Z"
    },
    "papermill": {
     "duration": 0.028267,
     "end_time": "2023-02-16T05:28:46.388984",
     "exception": false,
     "start_time": "2023-02-16T05:28:46.360717",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c8642b59",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-16T05:28:46.430831Z",
     "iopub.status.busy": "2023-02-16T05:28:46.430284Z",
     "iopub.status.idle": "2023-02-16T05:28:46.436551Z",
     "shell.execute_reply": "2023-02-16T05:28:46.435256Z"
    },
    "papermill": {
     "duration": 0.030951,
     "end_time": "2023-02-16T05:28:46.438850",
     "exception": false,
     "start_time": "2023-02-16T05:28:46.407899",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_punctuations(text):\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), \" \",text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b240eecc",
   "metadata": {
    "papermill": {
     "duration": 0.019366,
     "end_time": "2023-02-16T05:28:46.478325",
     "exception": false,
     "start_time": "2023-02-16T05:28:46.458959",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Another approach might be to only keep alphanumeric characters using regex pattern \"[^a-zA-Z0-9]\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1a709c49",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-16T05:28:46.519162Z",
     "iopub.status.busy": "2023-02-16T05:28:46.518770Z",
     "iopub.status.idle": "2023-02-16T05:28:46.565792Z",
     "shell.execute_reply": "2023-02-16T05:28:46.564596Z"
    },
    "papermill": {
     "duration": 0.070242,
     "end_time": "2023-02-16T05:28:46.568200",
     "exception": false,
     "start_time": "2023-02-16T05:28:46.497958",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2644    destruction so you have a new weapon that can ...\n",
       "2227    deluge the f   things i do for  gishwhes just ...\n",
       "5448    police dt   rt   the col police can catch a pi...\n",
       "132     aftershock aftershock back to school kick off ...\n",
       "6845    trauma in response to trauma children of addic...\n",
       "Name: tweet_noPuncts, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df[\"tweet_noPuncts\"] = tweets_df[\"tweet_noUnicode\"].apply(remove_punctuations)\n",
    "tweets_df[\"tweet_noPuncts\"].sample(5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15cfa8c",
   "metadata": {
    "papermill": {
     "duration": 0.019208,
     "end_time": "2023-02-16T05:28:46.607039",
     "exception": false,
     "start_time": "2023-02-16T05:28:46.587831",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Remove Digits or Words Containing Digits\n",
    "This might not be appropriate in many cases. For example \"MH370\" mentioned in the tweets corresponds to Malaysia Airlines Flight 370 which went missing. In this case, keeping this number in the text might be useful in the disaster tweet classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a0a43f0b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-16T05:28:46.648322Z",
     "iopub.status.busy": "2023-02-16T05:28:46.647905Z",
     "iopub.status.idle": "2023-02-16T05:28:46.652107Z",
     "shell.execute_reply": "2023-02-16T05:28:46.651407Z"
    },
    "papermill": {
     "duration": 0.027375,
     "end_time": "2023-02-16T05:28:46.654015",
     "exception": false,
     "start_time": "2023-02-16T05:28:46.626640",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_digits(text):\n",
    "    pattern = re.compile(\"\\w*\\d+\\w*\")\n",
    "    text = re.sub(pattern, \"\",text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e05e44a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-16T05:28:46.696322Z",
     "iopub.status.busy": "2023-02-16T05:28:46.695691Z",
     "iopub.status.idle": "2023-02-16T05:28:46.702576Z",
     "shell.execute_reply": "2023-02-16T05:28:46.701383Z"
    },
    "papermill": {
     "duration": 0.031268,
     "end_time": "2023-02-16T05:28:46.705335",
     "exception": false,
     "start_time": "2023-02-16T05:28:46.674067",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'    s of volcano hawaii'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \" m194 0104 utc5km s of volcano hawaii\"\n",
    "remove_digits(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a20d2fc4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-16T05:28:46.748846Z",
     "iopub.status.busy": "2023-02-16T05:28:46.748172Z",
     "iopub.status.idle": "2023-02-16T05:28:46.843936Z",
     "shell.execute_reply": "2023-02-16T05:28:46.841927Z"
    },
    "papermill": {
     "duration": 0.121406,
     "end_time": "2023-02-16T05:28:46.846829",
     "exception": false,
     "start_time": "2023-02-16T05:28:46.725423",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2644    destruction so you have a new weapon that can ...\n",
       "2227    deluge the f   things i do for  gishwhes just ...\n",
       "5448    police dt   rt   the col police can catch a pi...\n",
       "132     aftershock aftershock back to school kick off ...\n",
       "6845    trauma in response to trauma children of addic...\n",
       "Name: tweet_noDigits, dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df[\"tweet_noDigits\"] = tweets_df[\"tweet_noPuncts\"].apply(remove_digits)\n",
    "tweets_df[\"tweet_noDigits\"].sample(5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a644b46d",
   "metadata": {
    "papermill": {
     "duration": 0.020046,
     "end_time": "2023-02-16T05:28:46.886976",
     "exception": false,
     "start_time": "2023-02-16T05:28:46.866930",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Remove Stopwords\n",
    "Stopwords removal is one of the fundamental preprocessing operations in many NLP tasks. Stopwords are words like 'a, and, the, is, can' which are removed to only keep information rich words in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d10e82ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-16T05:28:46.930713Z",
     "iopub.status.busy": "2023-02-16T05:28:46.929363Z",
     "iopub.status.idle": "2023-02-16T05:28:46.942980Z",
     "shell.execute_reply": "2023-02-16T05:28:46.942192Z"
    },
    "papermill": {
     "duration": 0.037441,
     "end_time": "2023-02-16T05:28:46.945328",
     "exception": false,
     "start_time": "2023-02-16T05:28:46.907887",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'his', \"haven't\", 'in', 'will', 'what', 'down', 'after', 'whom', 'hadn', 'most', 'doing', 'with', 'ourselves', 'under', \"didn't\", 'not', 'doesn', \"that'll\", 'each', \"aren't\", 'o', 're', 'into', 'themselves', 'ma', 'these', 'myself', 'yours', 'we', 'yourselves', 'very', \"doesn't\", 'does', 'has', 'herself', 'no', 'mightn', 'm', 'up', \"mightn't\", 'for', 'about', 'have', \"you're\", 'to', 'an', 'd', 'having', 'this', \"hasn't\", 'being', 'were', 'above', 'now', 'wouldn', 'do', 'her', 'mustn', 'because', 'here', 'didn', 'once', 'before', 'your', 'wasn', 'more', 'is', 'off', 'further', 'aren', 'who', \"hadn't\", 'haven', 'both', \"won't\", 'that', \"wouldn't\", \"isn't\", 'all', 'should', 'it', 'can', \"wasn't\", 've', 'was', \"mustn't\", 'hers', 'ain', 'same', 'such', 'if', 'of', 'so', \"needn't\", \"she's\", 'are', 'on', 'than', 'the', 'don', \"should've\", 'been', 'their', 'any', 'too', 'its', 'my', 'between', 'why', 'ours', 'which', 'or', 'isn', 'won', 'out', 'how', 'i', 'where', 'only', 'as', 'll', 'until', 'those', 'had', 'few', 'during', 'theirs', 'am', 'from', 'a', 'then', 'you', 'through', 'below', 'some', 's', 'did', \"weren't\", 'himself', 'at', 'yourself', 'weren', 'shouldn', 'own', \"shouldn't\", 'while', 'itself', 'against', \"shan't\", \"you've\", 'nor', 't', 'him', \"don't\", \"you'll\", 'she', 'couldn', 'our', 'hasn', \"it's\", 'he', 'but', 'them', 'and', 'me', 'when', \"couldn't\", 'just', 'by', 'be', 'again', 'there', 'y', 'they', 'needn', 'shan', \"you'd\", 'over', 'other'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78074dd",
   "metadata": {
    "papermill": {
     "duration": 0.019886,
     "end_time": "2023-02-16T05:28:46.986275",
     "exception": false,
     "start_time": "2023-02-16T05:28:46.966389",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "NLTK library supports multiple languages and stopwords from these languages can be obtained by simply replacing 'english' with the name of the language in above code. The supported languages are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5a7ab6af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-16T05:28:47.028795Z",
     "iopub.status.busy": "2023-02-16T05:28:47.027823Z",
     "iopub.status.idle": "2023-02-16T05:28:47.034020Z",
     "shell.execute_reply": "2023-02-16T05:28:47.032573Z"
    },
    "papermill": {
     "duration": 0.029635,
     "end_time": "2023-02-16T05:28:47.036209",
     "exception": false,
     "start_time": "2023-02-16T05:28:47.006574",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['arabic', 'azerbaijani', 'basque', 'bengali', 'catalan', 'chinese', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'greek', 'hebrew', 'hinglish', 'hungarian', 'indonesian', 'italian', 'kazakh', 'nepali', 'norwegian', 'portuguese', 'romanian', 'russian', 'slovene', 'spanish', 'swedish', 'tajik', 'turkish']\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "157b4e4a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-16T05:28:47.077552Z",
     "iopub.status.busy": "2023-02-16T05:28:47.077186Z",
     "iopub.status.idle": "2023-02-16T05:28:47.083548Z",
     "shell.execute_reply": "2023-02-16T05:28:47.082188Z"
    },
    "papermill": {
     "duration": 0.029573,
     "end_time": "2023-02-16T05:28:47.085709",
     "exception": false,
     "start_time": "2023-02-16T05:28:47.056136",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    return \" \".join([word for word in str(text).split() if word not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c7bf11f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-16T05:28:47.127037Z",
     "iopub.status.busy": "2023-02-16T05:28:47.126616Z",
     "iopub.status.idle": "2023-02-16T05:28:47.160594Z",
     "shell.execute_reply": "2023-02-16T05:28:47.159644Z"
    },
    "papermill": {
     "duration": 0.057265,
     "end_time": "2023-02-16T05:28:47.162638",
     "exception": false,
     "start_time": "2023-02-16T05:28:47.105373",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2644    destruction new weapon cause un imaginable des...\n",
       "2227    deluge f things gishwhes got soaked deluge goi...\n",
       "5448    police dt rt col police catch pickpocket liver...\n",
       "132     aftershock aftershock back school kick great w...\n",
       "6845    trauma response trauma children addicts develo...\n",
       "Name: tweet_noStopwords, dtype: object"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df[\"tweet_noStopwords\"] = tweets_df[\"tweet_noDigits\"].apply(remove_stopwords)\n",
    "tweets_df[\"tweet_noStopwords\"].sample(5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13a03fa",
   "metadata": {
    "papermill": {
     "duration": 0.019928,
     "end_time": "2023-02-16T05:28:47.202638",
     "exception": false,
     "start_time": "2023-02-16T05:28:47.182710",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "I sometimes remove stopwords before removing punctuations as many stopwords contain apostrophe. However, most of these stopwprds are expanded during contraction expansion process above. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be35805",
   "metadata": {
    "papermill": {
     "duration": 0.02,
     "end_time": "2023-02-16T05:28:47.243334",
     "exception": false,
     "start_time": "2023-02-16T05:28:47.223334",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Removing Extra Spaces\n",
    "While performing different preprocessing steps, additional spaces are introduced in the text at the start, end or in-between words which need to be removed. In above case while removing stopwords we split the text using spaces which removes extra spaces. However, we can still run the following code to be sure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "78bae5e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-16T05:28:47.285739Z",
     "iopub.status.busy": "2023-02-16T05:28:47.285021Z",
     "iopub.status.idle": "2023-02-16T05:28:47.290294Z",
     "shell.execute_reply": "2023-02-16T05:28:47.288628Z"
    },
    "papermill": {
     "duration": 0.029364,
     "end_time": "2023-02-16T05:28:47.292633",
     "exception": false,
     "start_time": "2023-02-16T05:28:47.263269",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_extra_spaces(text):\n",
    "    text = re.sub(' +', ' ', text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "759b916d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-16T05:28:47.334792Z",
     "iopub.status.busy": "2023-02-16T05:28:47.333744Z",
     "iopub.status.idle": "2023-02-16T05:28:47.373491Z",
     "shell.execute_reply": "2023-02-16T05:28:47.371936Z"
    },
    "papermill": {
     "duration": 0.063203,
     "end_time": "2023-02-16T05:28:47.375740",
     "exception": false,
     "start_time": "2023-02-16T05:28:47.312537",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2644    destruction new weapon cause un imaginable des...\n",
       "2227    deluge f things gishwhes got soaked deluge goi...\n",
       "5448    police dt rt col police catch pickpocket liver...\n",
       "132     aftershock aftershock back school kick great w...\n",
       "6845    trauma response trauma children addicts develo...\n",
       "Name: tweet_noExtraspace, dtype: object"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df[\"tweet_noExtraspace\"] = tweets_df[\"tweet_noStopwords\"].apply(remove_extra_spaces)\n",
    "tweets_df[\"tweet_noExtraspace\"].sample(5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ed1f77",
   "metadata": {
    "papermill": {
     "duration": 0.020038,
     "end_time": "2023-02-16T05:28:47.416194",
     "exception": false,
     "start_time": "2023-02-16T05:28:47.396156",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Stemming or Lemmatization\n",
    "Stemming uses the stem of the word, while lemmatization uses the context in which the word is being used. However, stemming leads to incorrect meaning and spelling. Lemmatization gives meaningful words based on the context. Hence, I generally prefer lemmatization over stemming as lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d909a479",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-16T05:28:47.458699Z",
     "iopub.status.busy": "2023-02-16T05:28:47.458342Z",
     "iopub.status.idle": "2023-02-16T05:28:47.464700Z",
     "shell.execute_reply": "2023-02-16T05:28:47.463341Z"
    },
    "papermill": {
     "duration": 0.030153,
     "end_time": "2023-02-16T05:28:47.466716",
     "exception": false,
     "start_time": "2023-02-16T05:28:47.436563",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    words = [lemmatizer.lemmatize(word) for word in text.split()]\n",
    "    text = ' '.join(words)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a65a6f89",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-16T05:28:47.508016Z",
     "iopub.status.busy": "2023-02-16T05:28:47.507655Z",
     "iopub.status.idle": "2023-02-16T05:28:49.541801Z",
     "shell.execute_reply": "2023-02-16T05:28:49.540498Z"
    },
    "papermill": {
     "duration": 2.057906,
     "end_time": "2023-02-16T05:28:49.544692",
     "exception": false,
     "start_time": "2023-02-16T05:28:47.486786",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2644    destruction new weapon cause un imaginable des...\n",
       "2227    deluge f thing gishwhes got soaked deluge goin...\n",
       "5448    police dt rt col police catch pickpocket liver...\n",
       "132     aftershock aftershock back school kick great w...\n",
       "6845    trauma response trauma child addict develop de...\n",
       "Name: tweet_lemmatised, dtype: object"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df[\"tweet_lemmatised\"] = tweets_df[\"tweet_noExtraspace\"].apply(lemmatize_text)\n",
    "tweets_df[\"tweet_lemmatised\"].sample(5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbed1cd8",
   "metadata": {
    "papermill": {
     "duration": 0.020595,
     "end_time": "2023-02-16T05:28:49.586620",
     "exception": false,
     "start_time": "2023-02-16T05:28:49.566025",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Spelling Correction\n",
    "Spelling correction can help reduce the variations of the word and avoid missrepresentation of the information. It can help in the NLP task of tweet classification in the considered example because the tweets are particularly succeptible to incorrect spellings of words, either deliberate or otherwise. There are few options such as spell checker from TextBlob and Symspellpy (Python port of SymSpell). However, the Textblob is prohibitively slow while Symspellpy is very fast and accurate. Also Symspellpy is language agnostic if proper dictionary is used, hence is used here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "75f034af",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2023-02-16T05:28:49.630270Z",
     "iopub.status.busy": "2023-02-16T05:28:49.629755Z",
     "iopub.status.idle": "2023-02-16T05:28:59.915241Z",
     "shell.execute_reply": "2023-02-16T05:28:59.913837Z"
    },
    "papermill": {
     "duration": 10.310518,
     "end_time": "2023-02-16T05:28:59.917850",
     "exception": false,
     "start_time": "2023-02-16T05:28:49.607332",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting symspellpy\r\n",
      "  Downloading symspellpy-6.7.7-py3-none-any.whl (2.6 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting editdistpy>=0.1.3\r\n",
      "  Downloading editdistpy-0.1.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (125 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.5/125.5 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: editdistpy, symspellpy\r\n",
      "Successfully installed editdistpy-0.1.3 symspellpy-6.7.7\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install symspellpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4997632f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-16T05:28:59.962779Z",
     "iopub.status.busy": "2023-02-16T05:28:59.962371Z",
     "iopub.status.idle": "2023-02-16T05:28:59.976426Z",
     "shell.execute_reply": "2023-02-16T05:28:59.975515Z"
    },
    "papermill": {
     "duration": 0.039517,
     "end_time": "2023-02-16T05:28:59.978980",
     "exception": false,
     "start_time": "2023-02-16T05:28:59.939463",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pkg_resources\n",
    "from symspellpy import SymSpell, Verbosity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebde3d7",
   "metadata": {
    "papermill": {
     "duration": 0.021232,
     "end_time": "2023-02-16T05:29:00.021939",
     "exception": false,
     "start_time": "2023-02-16T05:29:00.000707",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "SymSpellpy give multiple suggestions to the words for spelling correction. We can select the first suggested word having highest probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "20f5ada5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-16T05:29:00.067280Z",
     "iopub.status.busy": "2023-02-16T05:29:00.066817Z",
     "iopub.status.idle": "2023-02-16T05:29:03.453260Z",
     "shell.execute_reply": "2023-02-16T05:29:03.452300Z"
    },
    "papermill": {
     "duration": 3.411542,
     "end_time": "2023-02-16T05:29:03.455218",
     "exception": false,
     "start_time": "2023-02-16T05:29:00.043676",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
    "dictionary_path = pkg_resources.resource_filename(\n",
    "    \"symspellpy\", \"frequency_dictionary_en_82_765.txt\"\n",
    ")\n",
    "sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "440631aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-16T05:29:03.499590Z",
     "iopub.status.busy": "2023-02-16T05:29:03.499135Z",
     "iopub.status.idle": "2023-02-16T05:29:03.505733Z",
     "shell.execute_reply": "2023-02-16T05:29:03.504278Z"
    },
    "papermill": {
     "duration": 0.0319,
     "end_time": "2023-02-16T05:29:03.508199",
     "exception": false,
     "start_time": "2023-02-16T05:29:03.476299",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def correct_spelling_symspell(text):\n",
    "    words = [\n",
    "        sym_spell.lookup(\n",
    "            word, \n",
    "            Verbosity.CLOSEST, \n",
    "            max_edit_distance=2,\n",
    "            include_unknown=True\n",
    "            )[0].term \n",
    "        for word in text.split()] \n",
    "    text = \" \".join(words)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84eaf67d",
   "metadata": {
    "papermill": {
     "duration": 0.021337,
     "end_time": "2023-02-16T05:29:03.551557",
     "exception": false,
     "start_time": "2023-02-16T05:29:03.530220",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The `include_unknown` option keeps the words not within `max_edit_distance` from the words in the dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dbe7ce16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-16T05:29:03.596469Z",
     "iopub.status.busy": "2023-02-16T05:29:03.595932Z",
     "iopub.status.idle": "2023-02-16T05:29:05.127506Z",
     "shell.execute_reply": "2023-02-16T05:29:05.126601Z"
    },
    "papermill": {
     "duration": 1.556441,
     "end_time": "2023-02-16T05:29:05.129476",
     "exception": false,
     "start_time": "2023-02-16T05:29:03.573035",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2644    destruction new weapon cause in imaginable des...\n",
       "2227    deluge of thing gishwhes got soaked deluge goi...\n",
       "5448    police it it col police catch pickpocket liver...\n",
       "132     aftershock aftershock back school kick great w...\n",
       "6845    trauma response trauma child addict develop de...\n",
       "Name: tweet_spellcheck, dtype: object"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df[\"tweet_spellcheck\"] = tweets_df[\"tweet_lemmatised\"].apply(correct_spelling_symspell)\n",
    "tweets_df[\"tweet_spellcheck\"].sample(5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830e7d2a",
   "metadata": {
    "papermill": {
     "duration": 0.020824,
     "end_time": "2023-02-16T05:29:05.171494",
     "exception": false,
     "start_time": "2023-02-16T05:29:05.150670",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "It can be observed that it is not perfect and introduces more stopwords but can help in many cases. Some more investigation is required with the competition solution results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1273b783",
   "metadata": {
    "papermill": {
     "duration": 0.021915,
     "end_time": "2023-02-16T05:29:05.215963",
     "exception": false,
     "start_time": "2023-02-16T05:29:05.194048",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The [symspellpy library](https://symspellpy.readthedocs.io/en/latest/examples/dictionary.html) is said be \"language independent (agnostic)\" and can be used with any language. The already available english dictionary is used in the above example, but such a dictionary can be easily created for any language using large enough text data in 'plain text' format using the `create_dictionary` function. You can read [1000x Faster Spelling Correction algorithm](https://wolfgarbe.medium.com/1000x-faster-spelling-correction-algorithm-2012-8701fcd87a5f) and the [documentation of symspellpy library](https://symspellpy.readthedocs.io/en/latest/) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ce742031",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-16T05:29:05.262261Z",
     "iopub.status.busy": "2023-02-16T05:29:05.261805Z",
     "iopub.status.idle": "2023-02-16T05:29:05.266848Z",
     "shell.execute_reply": "2023-02-16T05:29:05.265776Z"
    },
    "papermill": {
     "duration": 0.030504,
     "end_time": "2023-02-16T05:29:05.268670",
     "exception": false,
     "start_time": "2023-02-16T05:29:05.238166",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from symspellpy import SymSpell\n",
    "\n",
    "# sym_spell = SymSpell()\n",
    "# corpus_path = <path/to/plain/text/file>\n",
    "# sym_spell.create_dictionary(corpus_path)\n",
    "\n",
    "# print(sym_spell.words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de915d51",
   "metadata": {
    "papermill": {
     "duration": 0.020321,
     "end_time": "2023-02-16T05:29:05.309995",
     "exception": false,
     "start_time": "2023-02-16T05:29:05.289674",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Correcting Componded Words \n",
    "Sometimes multiple words are concatenated without space leading to words not available in the dictionary. These words result in misrepresentation of the information. Such compound words present in the tweets dataset are mostly resulting from the hashtags. As mentioned before, hashtags contain useful information and hence the compund words need to be segmented into meaningful words. Some hashtags are proper names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a4c51977",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-16T05:29:05.354037Z",
     "iopub.status.busy": "2023-02-16T05:29:05.353385Z",
     "iopub.status.idle": "2023-02-16T05:29:05.705670Z",
     "shell.execute_reply": "2023-02-16T05:29:05.704758Z"
    },
    "papermill": {
     "duration": 0.377526,
     "end_time": "2023-02-16T05:29:05.708364",
     "exception": false,
     "start_time": "2023-02-16T05:29:05.330838",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_path = pkg_resources.resource_filename(\n",
    "    \"symspellpy\", \"frequency_bigramdictionary_en_243_342.txt\"\n",
    ")\n",
    "sym_spell.load_bigram_dictionary(bigram_path, term_index=0, count_index=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7f21027c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-16T05:29:05.754474Z",
     "iopub.status.busy": "2023-02-16T05:29:05.753979Z",
     "iopub.status.idle": "2023-02-16T05:29:05.760809Z",
     "shell.execute_reply": "2023-02-16T05:29:05.759256Z"
    },
    "papermill": {
     "duration": 0.033191,
     "end_time": "2023-02-16T05:29:05.763490",
     "exception": false,
     "start_time": "2023-02-16T05:29:05.730299",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def correct_spelling_symspell_compound(text):\n",
    "    words = [\n",
    "        sym_spell.lookup_compound(\n",
    "            word, \n",
    "            max_edit_distance=2\n",
    "            )[0].term \n",
    "        for word in text.split()] \n",
    "    text = \" \".join(words)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3fd3f45d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-16T05:29:05.807989Z",
     "iopub.status.busy": "2023-02-16T05:29:05.807577Z",
     "iopub.status.idle": "2023-02-16T05:29:05.839949Z",
     "shell.execute_reply": "2023-02-16T05:29:05.838695Z"
    },
    "papermill": {
     "duration": 0.056918,
     "end_time": "2023-02-16T05:29:05.841930",
     "exception": false,
     "start_time": "2023-02-16T05:29:05.785012",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'iran deal panther attack trap music strategic patience social news as hurricane online communities human consumption'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"IranDeal PantherAttack TrapMusic StrategicPatience socialnews NASAHurricane onlinecommunities humanconsumption\"\n",
    "correct_spelling_symspell_compound(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ad2cfda5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-16T05:29:05.887670Z",
     "iopub.status.busy": "2023-02-16T05:29:05.886981Z",
     "iopub.status.idle": "2023-02-16T05:29:10.357187Z",
     "shell.execute_reply": "2023-02-16T05:29:10.356142Z"
    },
    "papermill": {
     "duration": 4.495711,
     "end_time": "2023-02-16T05:29:10.359356",
     "exception": false,
     "start_time": "2023-02-16T05:29:05.863645",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2644    destruction new weapon cause in imaginable des...\n",
       "2227    deluge of thing gish hes got soaked deluge goi...\n",
       "5448    police it it col police catch pickpocket liver...\n",
       "132     aftershock aftershock back school kick great w...\n",
       "6845    trauma response trauma child addict develop de...\n",
       "Name: tweet_spellcheck_compound, dtype: object"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df[\"tweet_spellcheck_compound\"] = tweets_df[\"tweet_spellcheck\"].apply(correct_spelling_symspell_compound)\n",
    "tweets_df[\"tweet_spellcheck_compound\"].sample(5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a225b4ea",
   "metadata": {
    "papermill": {
     "duration": 0.022252,
     "end_time": "2023-02-16T05:29:10.404036",
     "exception": false,
     "start_time": "2023-02-16T05:29:10.381784",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Britsh-American English Conversion\n",
    "There are many words in British and American english which differ in spellings such as (colour: color), (standardize: standardise) and so on. Depending upon the text data, the words from both of them can be present and one might want to convert all British english words to American words or vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "92034806",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-16T05:29:10.450296Z",
     "iopub.status.busy": "2023-02-16T05:29:10.449663Z",
     "iopub.status.idle": "2023-02-16T05:29:11.045893Z",
     "shell.execute_reply": "2023-02-16T05:29:11.044605Z"
    },
    "papermill": {
     "duration": 0.622734,
     "end_time": "2023-02-16T05:29:11.048773",
     "exception": false,
     "start_time": "2023-02-16T05:29:10.426039",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url =\"https://raw.githubusercontent.com/hyperreality/American-British-English-Translator/master/data/american_spellings.json\"\n",
    "american_to_british_dict = requests.get(url).json()\n",
    "\n",
    "url =\"https://raw.githubusercontent.com/hyperreality/American-British-English-Translator/master/data/british_spellings.json\"\n",
    "british_to_american_dict = requests.get(url).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3dc6511b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-16T05:29:11.094509Z",
     "iopub.status.busy": "2023-02-16T05:29:11.094068Z",
     "iopub.status.idle": "2023-02-16T05:29:11.101845Z",
     "shell.execute_reply": "2023-02-16T05:29:11.099727Z"
    },
    "papermill": {
     "duration": 0.03291,
     "end_time": "2023-02-16T05:29:11.103795",
     "exception": false,
     "start_time": "2023-02-16T05:29:11.070885",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Based on https://stackoverflow.com/questions/42329766/python-nlp-british-english-vs-american-english\n",
    "def britishize(text):\n",
    "    text = [american_to_british_dict[word] if word in american_to_british_dict else word for word in text.split()]\n",
    "    return \" \".join(text)\n",
    "\n",
    "\n",
    "def americanize(text):\n",
    "    text = [british_to_american_dict[word] if word in british_to_american_dict else word for word in text.split()]   \n",
    "    return \" \".join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "988613cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-16T05:29:11.150044Z",
     "iopub.status.busy": "2023-02-16T05:29:11.149408Z",
     "iopub.status.idle": "2023-02-16T05:29:11.155015Z",
     "shell.execute_reply": "2023-02-16T05:29:11.154097Z"
    },
    "papermill": {
     "duration": 0.031363,
     "end_time": "2023-02-16T05:29:11.156977",
     "exception": false,
     "start_time": "2023-02-16T05:29:11.125614",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Discount analyze standardized color'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Discount analyse standardised colour\"\n",
    "americanize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c3ac896d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-16T05:29:11.203678Z",
     "iopub.status.busy": "2023-02-16T05:29:11.203243Z",
     "iopub.status.idle": "2023-02-16T05:29:11.210218Z",
     "shell.execute_reply": "2023-02-16T05:29:11.208666Z"
    },
    "papermill": {
     "duration": 0.033526,
     "end_time": "2023-02-16T05:29:11.212216",
     "exception": false,
     "start_time": "2023-02-16T05:29:11.178690",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'Discount analyse standardised color'\""
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"'Discount analyze standardized color'\"\n",
    "britishize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6bd6be47",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-16T05:29:11.258676Z",
     "iopub.status.busy": "2023-02-16T05:29:11.258276Z",
     "iopub.status.idle": "2023-02-16T05:29:11.286823Z",
     "shell.execute_reply": "2023-02-16T05:29:11.285900Z"
    },
    "papermill": {
     "duration": 0.054463,
     "end_time": "2023-02-16T05:29:11.288960",
     "exception": false,
     "start_time": "2023-02-16T05:29:11.234497",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2644    destruction new weapon cause in imaginable des...\n",
       "2227    deluge of thing gish hes got soaked deluge goi...\n",
       "5448    police it it col police catch pickpocket liver...\n",
       "132     aftershock aftershock back school kick great w...\n",
       "6845    trauma response trauma child addict develop de...\n",
       "Name: tweet_american, dtype: object"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df[\"tweet_american\"] = tweets_df[\"tweet_spellcheck_compound\"].apply(americanize)\n",
    "tweets_df[\"tweet_american\"].sample(5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3ce3be72",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-16T05:29:11.335549Z",
     "iopub.status.busy": "2023-02-16T05:29:11.335090Z",
     "iopub.status.idle": "2023-02-16T05:29:11.363335Z",
     "shell.execute_reply": "2023-02-16T05:29:11.362126Z"
    },
    "papermill": {
     "duration": 0.053936,
     "end_time": "2023-02-16T05:29:11.365270",
     "exception": false,
     "start_time": "2023-02-16T05:29:11.311334",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2644    destruction new weapon cause in imaginable des...\n",
       "2227    deluge of thing gish hes got soaked deluge goi...\n",
       "5448    police it it col police catch pickpocket liver...\n",
       "132     aftershock aftershock back school kick great w...\n",
       "6845    trauma response trauma child addict develop de...\n",
       "Name: tweet_british, dtype: object"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df[\"tweet_british\"] = tweets_df[\"tweet_spellcheck_compound\"].apply(britishize)\n",
    "tweets_df[\"tweet_british\"].sample(5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ff8e25",
   "metadata": {
    "papermill": {
     "duration": 0.021872,
     "end_time": "2023-02-16T05:29:11.409410",
     "exception": false,
     "start_time": "2023-02-16T05:29:11.387538",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Final Stopward Removal\n",
    "Due to previous spell checking and compound word segmentation steps, few new stopwords are introduced in the data and hence one final stopward removal step is required. Additional words can be included in the stopwords list based on specific application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "139826ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-16T05:29:11.455324Z",
     "iopub.status.busy": "2023-02-16T05:29:11.454904Z",
     "iopub.status.idle": "2023-02-16T05:29:11.481011Z",
     "shell.execute_reply": "2023-02-16T05:29:11.480335Z"
    },
    "papermill": {
     "duration": 0.051468,
     "end_time": "2023-02-16T05:29:11.482781",
     "exception": false,
     "start_time": "2023-02-16T05:29:11.431313",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2644    destruction new weapon cause imaginable destru...\n",
       "2227    deluge thing gish hes got soaked deluge going ...\n",
       "5448    police col police catch pickpocket liverpool s...\n",
       "132     aftershock aftershock back school kick great w...\n",
       "6845    trauma response trauma child addict develop de...\n",
       "Name: tweet_final, dtype: object"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df[\"tweet_final\"] = tweets_df[\"tweet_spellcheck_compound\"].apply(remove_stopwords)\n",
    "tweets_df[\"tweet_final\"].sample(5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1b54ed36",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-16T05:29:11.529940Z",
     "iopub.status.busy": "2023-02-16T05:29:11.528694Z",
     "iopub.status.idle": "2023-02-16T05:29:11.752569Z",
     "shell.execute_reply": "2023-02-16T05:29:11.751793Z"
    },
    "papermill": {
     "duration": 0.250171,
     "end_time": "2023-02-16T05:29:11.755151",
     "exception": false,
     "start_time": "2023-02-16T05:29:11.504980",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tweets_df.to_csv(\"distaster_tweets_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95605481",
   "metadata": {
    "papermill": {
     "duration": 0.022028,
     "end_time": "2023-02-16T05:29:11.799462",
     "exception": false,
     "start_time": "2023-02-16T05:29:11.777434",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Do check the csv file generated after these steps in some external software like MS Excel or Google Docs to better understand the effects of each preprocessing step on the input text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e737ceca",
   "metadata": {
    "papermill": {
     "duration": 0.021515,
     "end_time": "2023-02-16T05:29:11.843856",
     "exception": false,
     "start_time": "2023-02-16T05:29:11.822341",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Sequence of Preprocessing Steps\n",
    "Proper sequence of these operations need to be determined to achieve higher efficiency of data preprocessing\n",
    "\n",
    "(will be added soon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a4f67a",
   "metadata": {
    "papermill": {
     "duration": 0.021488,
     "end_time": "2023-02-16T05:29:11.887129",
     "exception": false,
     "start_time": "2023-02-16T05:29:11.865641",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Classification using Clean Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bd73f8",
   "metadata": {
    "papermill": {
     "duration": 0.021654,
     "end_time": "2023-02-16T05:29:11.930369",
     "exception": false,
     "start_time": "2023-02-16T05:29:11.908715",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "I will add few examples of training multiple simple models and compare the results soon"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 53.923313,
   "end_time": "2023-02-16T05:29:13.178408",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-02-16T05:28:19.255095",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
